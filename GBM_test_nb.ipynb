{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Simulates a multivariable geometric brownian motion with 3 variables, and performs inference on\n",
    "the drift parameters, before outputting metrics for the posterior distributions.\n",
    "\n",
    "First, import the relevant packages"
   ],
   "id": "ad65b4dccc8cd568"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    # set this to \"torch\", \"tensorflow\", or \"jax\"\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "import os\n",
    "import jax"
   ],
   "id": "c446b974fcd602b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we define our global random number generator, as well as the DRIFT_SCALE parameter, which is used to control the general direction of each path (in the context of a stock, it can be thought of as the expected return)",
   "id": "211a6ed4fef4409c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "RNG = np.random.default_rng(int(os.times()[4]))\n",
    "DRIFT_SCALE = 0.4"
   ],
   "id": "f39fae385d11902d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We then define a function to get a random draw from our prior distribution of the drift parameters, which we model here as a uniform random variable from -DRIFT_SCALE to DRIFT_SCALE",
   "id": "e15b4ca56a2fc6ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prior():\n",
    "    # Generates a random draw from the prior\n",
    "\n",
    "    b1 = RNG.uniform(-DRIFT_SCALE, DRIFT_SCALE)\n",
    "    b2 = RNG.uniform(-DRIFT_SCALE, DRIFT_SCALE)\n",
    "    b3 = RNG.uniform(-DRIFT_SCALE, DRIFT_SCALE)\n",
    "\n",
    "    return {\"b1\":b1, \"b2\":b2, \"b3\":b3}"
   ],
   "id": "daf0e7840ea0ca20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We then define our simulator, which generates a simulation over a given time length in the specified number of time steps using the formula for a multivariate geometric brownian motion. This is then outputted as a dictionary to be passed on in our workflow",
   "id": "2d8eba2dffa34e26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def GBM_sim(b1, b2, b3, x0 = np.array([100, 100, 100]), time = 100/365, time_step = 1/365):\n",
    "    sigma = np.array([[0.5, 0.1, 0.0],\n",
    "                      [0.0, 0.1, 0.3],\n",
    "                      [0.0, 0.0, 0.2]])\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    motion = [x0]\n",
    "\n",
    "    for _ in range(0, int(time/time_step) - 1):\n",
    "        drift_coef = np.array([b1, b2, b3])\n",
    "        correction = 0.5*np.sum([sigma[:, j]**2 for j in range(0, 3)], axis=0)\n",
    "        drift = drift_coef - correction\n",
    "        timescaled_drift = drift * time_step\n",
    "        random_shocks = sigma @ RNG.normal(scale=np.sqrt(time_step), size=3)\n",
    "        dx = x * (timescaled_drift + random_shocks)\n",
    "        x = x + dx\n",
    "        motion.append(x)\n",
    "\n",
    "    return dict(motion=np.asarray(motion))"
   ],
   "id": "98f5ac045fe69e93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, in preparation for performing bayesian inference, we use a Gated Recurrent Unit (GRU) to process our time-series data into a format usable for our network",
   "id": "f2219c488cae54d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GRU(bf.networks.SummaryNetwork):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.gru = keras.layers.GRU(128, dropout=0.1)\n",
    "        self.summary_stats = keras.layers.Dense(64)\n",
    "\n",
    "    def call(self, time_series, **kwargs):\n",
    "        summary = self.gru(time_series, training=kwargs.get(\"stage\") == \"training\")\n",
    "        summary = self.summary_stats(summary)\n",
    "        return summary"
   ],
   "id": "7efcc7360e4ca985"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, lets run bayesian inference on this simulator using a CouplingFlow inference network and plot our learned distributions. First, we define the components of our network: the simulator (the combination of our prior generation function and the simulator that generates data with those priors); the adapter (which formats the data to be input into our network); the summary network (transforms the summary variables into a usable format); and the inference network (to actually perform the amortized inference)",
   "id": "38fdd003f497b676"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "simulator = bf.simulators.make_simulator([prior, GBM_sim])\n",
    "\n",
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .as_time_series(\"motion\")\n",
    "    .concatenate([\"b1\", \"b2\", \"b3\"], into=\"inference_variables\")\n",
    "    .rename(\"motion\", \"summary_variables\")\n",
    "    .log([\"inference_variables\", \"summary_variables\"], p1=True)\n",
    ")\n",
    "\n",
    "summary_net = GRU()\n",
    "\n",
    "inference_net = bf.networks.CouplingFlow()\n",
    "\n",
    "workflow = bf.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    summary_network=summary_net,\n",
    "    inference_network=inference_net,\n",
    "    standardize=None\n",
    ")"
   ],
   "id": "c6da0a2d0f8f6424"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then it's time to train our network, so we create training and validation datasets, which we train our network on, and plot the loss function.",
   "id": "4c651b57b6bdc10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train = workflow.simulate(8000)\n",
    "validation = workflow.simulate(300)\n",
    "\n",
    "history = workflow.fit_offline(data=train,\n",
    "                               epochs=100,\n",
    "                               batch_size=64,\n",
    "                               validation_data=validation)\n",
    "\n",
    "f = bf.diagnostics.plots.loss(history)\n",
    "\n",
    "plt.show()"
   ],
   "id": "8fab9b865d1af6b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, lets test the inference network by sampling the posterior distribution for a given input simulated input and seeing what distributions we learned for our three drift parameters vs the ground truth",
   "id": "469a8f614b5e10fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_datasets = 300\n",
    "num_samples = 1000\n",
    "\n",
    "# Simulate 300 scenarios\n",
    "test_sims = workflow.simulate(num_datasets)\n",
    "\n",
    "# Obtain num_samples posterior samples per scenario\n",
    "samples = workflow.sample(conditions=test_sims, num_samples=num_samples)\n",
    "\n",
    "f = bf.diagnostics.plots.recovery(samples, test_sims)\n",
    "\n",
    "b1_truth = test_sims[\"b1\"][0].item()\n",
    "b2_truth = test_sims[\"b2\"][0].item()\n",
    "b3_truth = test_sims[\"b3\"][0].item()\n",
    "truths = np.asarray([b1_truth, b2_truth, b3_truth])\n",
    "\n",
    "b1_samples = samples[\"b1\"][0].flatten()\n",
    "b2_samples = samples[\"b2\"][0].flatten()\n",
    "b3_samples = samples[\"b3\"][0].flatten()\n",
    "out_samples = np.asarray([b1_samples, b2_samples, b3_samples]).T\n",
    "\n",
    "labels = [\"b1\", \"b2\", \"b3\"]\n",
    "\n",
    "d = out_samples.shape[1]\n",
    "fig, axes = plt.subplots(d, d, figsize=(8, 8))\n",
    "\n",
    "for i in range(d):\n",
    "    for j in range(d):\n",
    "        ax = axes[i, j]\n",
    "        if i == j:\n",
    "            ax.set_facecolor(\"white\")  # set background blue\n",
    "            ax.hist(out_samples[:, i], bins=40, histtype=\"step\", color=\"lightblue\")\n",
    "            ax.axvline(truths[i], color=\"red\")\n",
    "            ax.set_xlabel(labels[i])\n",
    "        elif i < j:\n",
    "            ax.set_facecolor(\"midnightblue\")  # set background blue\n",
    "            h = ax.hist2d(out_samples[:, j], out_samples[:, i],\n",
    "                          bins=50, cmap=\"viridis\")\n",
    "            ax.plot(truths[j], truths[i], \"o\", color=\"red\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c69f08e2fdbc7b97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
